# Artifacts Directory

## Purpose

This directory stores artifacts generated by agent workflows — structured outputs, reports, generated code, analysis results, and intermediate work products.

## Why Artifacts?

Based on OpenAI's Shell + Skills + Compaction guidance, this follows the pattern:
- **Tools write to disk** (artifacts are created here)
- **Models reason over disk** (agent reviews/processes artifacts)
- **Developers retrieve from disk** (humans can access outputs)

Think of `/mnt/data` in OpenAI's hosted shell — this is our local equivalent.

## Structure

```
artifacts/
├── README.md              # This file
├── reports/              # Generated reports (audits, summaries, analyses)
├── code/                 # Generated code, scripts, prototypes
├── data/                 # Processed data files, cleaned datasets
├── images/               # Screenshots, generated images
└── temp/                 # Temporary work (can be deleted)
```

## Best Practices

1. **Meaningful names:** `skill-audit-2026-02-12.md` > `output.md`
2. **Timestamps:** Include date in filename for versioned artifacts
3. **Subdirectories:** Use category folders (reports/, code/, etc.)
4. **Cleanup:** Move old artifacts to archive/ or delete when no longer needed
5. **Documentation:** Each artifact should be self-documenting with clear purpose

## Handoff Boundary

When an agent completes work:
1. Write final output to artifacts/
2. Return the path to the user
3. User can review, edit, or feed into next step

This creates a clean separation between reasoning (agent) and deliverables (artifacts).

## Examples

```bash
# Generate a report
python analyze.py > artifacts/reports/api-audit-2026-02-12.json

# Create a prototype
agent creates artifacts/code/prototype-v1.html

# Process data
clean_dataset.py --input raw.csv --output artifacts/data/cleaned.csv
```

## Gitignore

Consider adding to `.gitignore` if artifacts are large or sensitive:
```
artifacts/temp/
artifacts/**/*.log
```
